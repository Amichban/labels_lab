#!/usr/bin/env python3
"""
Simple validation test script to verify the validation framework works.
This bypasses pytest configuration issues for basic testing.
"""

import sys
import os
from datetime import datetime, timedelta
from unittest.mock import Mock

# Add project root to path
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# Mock dependencies first
class MockSettings:
    clickhouse_host = "localhost"
    clickhouse_port = 8123
    clickhouse_user = "default"
    clickhouse_password = "password"
    clickhouse_database = "test_db"
    redis_url = "redis://localhost:6379"

# Mock modules
mock_settings = MockSettings()
sys.modules['config.settings'] = Mock()
sys.modules['config.settings'].settings = mock_settings
sys.modules['src.services.clickhouse_service'] = Mock()
sys.modules['src.services.clickhouse_service'].clickhouse_service = Mock()
sys.modules['src.services.redis_cache'] = Mock()
sys.modules['src.services.redis_cache'].redis_cache = Mock()

# Now import our validation modules
from src.validation.label_validator import LabelValidator, ValidationSeverity, ValidationCategory
from src.models.data_models import Candle, LabelSet, Granularity, EnhancedTripleBarrierLabel, BarrierHit


def test_basic_validation():
    """Test basic validation functionality"""
    print("üß™ Testing basic validation functionality...")
    
    validator = LabelValidator()
    
    # Test 1: Valid H4 candle
    print("\n1Ô∏è‚É£ Testing valid H4 candle...")
    valid_candle = Candle(
        instrument_id="EUR/USD",
        granularity=Granularity.H4,
        ts=datetime(2024, 1, 15, 9, 0, 0),  # Valid H4 timestamp
        open=1.0500,
        high=1.0580,
        low=1.0450,
        close=1.0520,
        volume=1000.0
    )
    
    result = validator.validate_pre_computation(valid_candle, 6)
    print(f"   Valid candle validation: {'‚úÖ PASS' if result.is_valid else '‚ùå FAIL'}")
    if not result.is_valid:
        for issue in result.issues:
            print(f"   Issue: {issue.severity.value} - {issue.message}")
    
    # Test 2: Invalid H4 timestamp
    print("\n2Ô∏è‚É£ Testing invalid H4 timestamp...")
    invalid_candle = Candle(
        instrument_id="EUR/USD",
        granularity=Granularity.H4,
        ts=datetime(2024, 1, 15, 8, 0, 0),  # Invalid H4 timestamp (should be 9:00)
        open=1.0500,
        high=1.0580,
        low=1.0450,
        close=1.0520,
        volume=1000.0
    )
    
    result = validator.validate_pre_computation(invalid_candle, 6)
    print(f"   Invalid timestamp validation: {'‚ùå FAIL (Expected)' if not result.is_valid else '‚ùå SHOULD HAVE FAILED'}")
    
    alignment_issues = [i for i in result.issues if i.category == ValidationCategory.TIMESTAMP_ALIGNMENT]
    print(f"   Found {len(alignment_issues)} timestamp alignment issues")
    
    # Test 3: Invalid OHLC relationships
    print("\n3Ô∏è‚É£ Testing invalid OHLC relationships...")
    invalid_ohlc_candle = Candle(
        instrument_id="EUR/USD",
        granularity=Granularity.H4,
        ts=datetime(2024, 1, 15, 9, 0, 0),
        open=1.0500,
        high=1.0450,  # High < Open - INVALID
        low=1.0550,   # Low > Open - INVALID
        close=1.0520,
        volume=1000.0
    )
    
    result = validator.validate_pre_computation(invalid_ohlc_candle, 6)
    print(f"   Invalid OHLC validation: {'‚ùå FAIL (Expected)' if not result.is_valid else '‚ùå SHOULD HAVE FAILED'}")
    
    consistency_issues = [i for i in result.issues if i.category == ValidationCategory.DATA_CONSISTENCY]
    print(f"   Found {len(consistency_issues)} data consistency issues")
    
    return True


def test_post_computation_validation():
    """Test post-computation validation"""
    print("\nüß™ Testing post-computation validation...")
    
    validator = LabelValidator()
    
    # Valid candle
    candle = Candle(
        instrument_id="EUR/USD",
        granularity=Granularity.H4,
        ts=datetime(2024, 1, 15, 9, 0, 0),
        open=1.0500,
        high=1.0580,
        low=1.0450,
        close=1.0520,
        volume=1000.0
    )
    
    # Test 1: Valid label set
    print("\n1Ô∏è‚É£ Testing valid label set...")
    valid_label_set = LabelSet(
        instrument_id=candle.instrument_id,
        granularity=candle.granularity,
        ts=candle.ts,
        enhanced_triple_barrier=EnhancedTripleBarrierLabel(
            label=1,
            barrier_hit=BarrierHit.UPPER,
            time_to_barrier=5,
            barrier_price=1.0580,
            level_adjusted=False,
            upper_barrier=1.0580,
            lower_barrier=1.0460
        ),
        forward_return=0.0057,
        vol_scaled_return=1.267,
        mfe=0.0060,
        mae=-0.0030,
        profit_factor=2.0,
        computation_time_ms=150.5
    )
    
    result = validator.validate_post_computation(candle, valid_label_set)
    print(f"   Valid label set validation: {'‚úÖ PASS' if result.is_valid else '‚ùå FAIL'}")
    if not result.is_valid:
        for issue in result.issues:
            print(f"   Issue: {issue.severity.value} - {issue.message}")
    
    # Test 2: MFE/MAE constraint violation
    print("\n2Ô∏è‚É£ Testing MFE/MAE constraint violation...")
    invalid_label_set = LabelSet(
        instrument_id=candle.instrument_id,
        granularity=candle.granularity,
        ts=candle.ts,
        mfe=0.0020,   # MFE = 0.002
        mae=-0.0030,  # MAE = -0.003, so -MAE = 0.003
                     # MFE (0.002) < -MAE (0.003) - CRITICAL VIOLATION
        profit_factor=2.0
    )
    
    result = validator.validate_post_computation(candle, invalid_label_set)
    print(f"   MFE/MAE violation validation: {'‚ùå FAIL (Expected)' if not result.is_valid else '‚ùå SHOULD HAVE FAILED'}")
    
    critical_issues = [i for i in result.issues if i.severity == ValidationSeverity.CRITICAL]
    print(f"   Found {len(critical_issues)} critical issues")
    
    return True


def test_look_ahead_bias_detection():
    """Test look-ahead bias detection"""
    print("\nüß™ Testing look-ahead bias detection...")
    
    validator = LabelValidator()
    
    # Valid candle
    candle_ts = datetime(2024, 1, 15, 9, 0, 0)
    candle = Candle(
        instrument_id="EUR/USD",
        granularity=Granularity.H4,
        ts=candle_ts,
        open=1.0500,
        high=1.0580,
        low=1.0450,
        close=1.0520,
        volume=1000.0
    )
    
    # Path data with look-ahead bias
    lookahead_path_data = [
        {
            "ts": candle_ts - timedelta(hours=1),  # FUTURE DATA - INVALID
            "open": 1.0520, "high": 1.0540, "low": 1.0510, "close": 1.0535
        },
        {
            "ts": candle_ts,
            "open": 1.0535, "high": 1.0560, "low": 1.0520, "close": 1.0550
        }
    ]
    
    result = validator.validate_pre_computation(candle, 6, path_data=lookahead_path_data)
    print(f"   Look-ahead bias detection: {'‚ùå FAIL (Expected)' if not result.is_valid else '‚ùå SHOULD HAVE FAILED'}")
    
    lookahead_issues = [i for i in result.issues if i.category == ValidationCategory.LOOKAHEAD_BIAS]
    print(f"   Found {len(lookahead_issues)} look-ahead bias issues")
    
    return True


def test_performance():
    """Test validation performance"""
    print("\nüß™ Testing validation performance...")
    
    validator = LabelValidator()
    
    # Create 100 valid candles
    candles = []
    base_time = datetime(2024, 1, 15, 1, 0, 0)
    
    for i in range(100):
        candles.append(Candle(
            instrument_id="EUR/USD",
            granularity=Granularity.H4,
            ts=base_time + timedelta(hours=4 * i),
            open=1.0500,
            high=1.0580,
            low=1.0450,
            close=1.0520,
            volume=1000.0
        ))
    
    # Time the validation
    start_time = datetime.now()
    
    for candle in candles:
        result = validator.validate_pre_computation(candle, 6)
    
    end_time = datetime.now()
    total_time_ms = (end_time - start_time).total_seconds() * 1000
    avg_time_per_validation = total_time_ms / len(candles)
    
    print(f"   Validated {len(candles)} candles in {total_time_ms:.2f}ms")
    print(f"   Average time per validation: {avg_time_per_validation:.2f}ms")
    
    # Performance should be reasonable (< 10ms per validation)
    performance_ok = avg_time_per_validation < 10
    print(f"   Performance test: {'‚úÖ PASS' if performance_ok else '‚ùå FAIL'}")
    
    return performance_ok


def main():
    """Run all validation tests"""
    print("üöÄ Starting validation framework tests...")
    print("=" * 60)
    
    tests_passed = 0
    total_tests = 0
    
    try:
        total_tests += 1
        if test_basic_validation():
            tests_passed += 1
    except Exception as e:
        print(f"‚ùå Basic validation test failed: {e}")
    
    try:
        total_tests += 1
        if test_post_computation_validation():
            tests_passed += 1
    except Exception as e:
        print(f"‚ùå Post-computation validation test failed: {e}")
    
    try:
        total_tests += 1
        if test_look_ahead_bias_detection():
            tests_passed += 1
    except Exception as e:
        print(f"‚ùå Look-ahead bias detection test failed: {e}")
    
    try:
        total_tests += 1
        if test_performance():
            tests_passed += 1
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")
    
    print("\n" + "=" * 60)
    print(f"üèÅ Test Results: {tests_passed}/{total_tests} tests passed")
    
    if tests_passed == total_tests:
        print("‚úÖ All tests PASSED! Validation framework is working correctly.")
        return 0
    else:
        print(f"‚ùå {total_tests - tests_passed} tests FAILED!")
        return 1


if __name__ == "__main__":
    sys.exit(main())